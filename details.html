<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1.0">
<title>Megapad-64 Tile Engine — Cycle & Layout Feasibility (32 MiB)</title>
<style>
/* ———  TYPO / LAYOUT  ——————————————————————————— */
body{
  font-family:-apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,"Helvetica Neue",Arial,sans-serif;
  line-height:1.55;
  margin:0;
  padding:0 1rem;
  background:#f6f8fb;
  color:#1b1e23;
}
h1,h2,h3{
  font-weight:600;
  color:#0d1117;
  margin:2rem 0 1rem;
}
h1{
  font-size:2rem;
  border-bottom:3px solid #3a5bff;
  padding-bottom:.5rem;
  margin-top:1.5rem;
}
h2{
  font-size:1.4rem;
  border-bottom:2px solid #d0d7de;
  padding-bottom:.3rem;
}
h3{
  font-size:1.1rem;
}
p{
  margin:0.5rem 0;
}
ul,ol{
  margin:0.5rem 0 0.5rem 1.5rem;
}
table{
  width:100%;
  border-collapse:collapse;
  margin:1rem 0;
  font-size:.9rem;
}
th,td{
  border:1px solid #d0d7de;
  padding:.4rem .6rem;
  text-align:left;
  vertical-align:top;
}
thead th{
  background:#e6edf3;
}
code,pre{
  font-family:"SFMono-Regular",Consolas,"Liberation Mono",Menlo,monospace;
  font-size:.82rem;
}
pre{
  background:#0d1117;
  color:#e6edf3;
  padding:1rem;
  border-radius:6px;
  overflow:auto;
}
aside.note{
  background:#fff9c4;
  border-left:4px solid #f5c940;
  padding:.75rem;
  margin:1.25rem 0;
}
aside.tip{
  background:#e6f7ea;
  border-left:4px solid #46c36f;
  padding:.75rem;
  margin:1.25rem 0;
}
.legal{
  font-size:.8rem;
  color:#57606a;
  margin:2.5rem 0;
}
@media (min-width:900px){
  .col2{
    display:grid;
    grid-template-columns:1fr 1fr;
    gap:1rem;
  }
}
</style>
</head>
<body>

<h1>Megapad-64 Tile Engine — Cycle &amp; Layout Feasibility<br>
<span style="font-weight:400;font-size:1.1rem">Focus: 32 MiB Megapad (16×2 MiB) configuration</span></h1>

<aside class="note">
<strong>Goal:</strong> Show that the Megapad-64 tile engine can plausibly sustain the advertised
<em>“1 tile cycle⁻¹ (512-bit datapath)”</em> throughput and fit cleanly into a realistic floorplan, using a
32 MiB Megapad configuration (16 banks × 2 MiB).
</aside>

<!-- ——— 1  ASSUMPTIONS & TARGETS ——————————————— -->
<h2 id="targets">1 Assumptions and Performance Targets</h2>

<table>
<thead>
  <tr><th>Item</th><th>Value</th><th>Notes</th></tr>
</thead>
<tbody>
  <tr>
    <td>Tile size</td>
    <td>64 bytes (8×8 grid of bytes)</td>
    <td>Row-major; same layout as main spec.</td>
  </tr>
  <tr>
    <td>Tile datapath width</td>
    <td>512 bit</td>
    <td>64 lanes × 8-bit; grouped for 16/32/64-bit lanes.</td>
  </tr>
  <tr>
    <td>Megapad size</td>
    <td>32 MiB</td>
    <td>16 logical banks × 2 MiB per bank.</td>
  </tr>
  <tr>
    <td>Tile operations</td>
    <td>TALU / TRED / TRANS / LOADC / ZERO</td>
    <td>1-cycle latency target.</td>
  </tr>
  <tr>
    <td>Heavier tile ops</td>
    <td>MUL (2 cycles), DOT / CONV3 (4 cycles)</td>
    <td>Pipelined; still ≤ 1 tile cycle⁻¹ throughput.</td>
  </tr>
  <tr>
    <td>Clock target</td>
    <td>≈1 GHz in 7 nm</td>
    <td>Fully static core; Megapad and MEX share clock domain.</td>
  </tr>
</tbody>
</table>

<aside class="tip">
The “1-cycle tile op” claim applies to <em>compute on tiles already resident in the tile engine</em>.
Tile loads/stores from Megapad use <code>LD1D/LD2D/ST1D/ST2D</code> and may span multiple cycles, but can be overlapped to sustain 1 tile per cycle once the pipeline is full.
</aside>

<!-- ——— 2  TILE ALU MICRO-ARCH ——————————————— -->
<h2 id="tile-alu">2 Tile ALU Micro-Architecture (Cycle Feasibility)</h2>

<h3>2.1 Lane structure and element widths</h3>
<p>
Each tile is 64 bytes. Element widths and lane counts:
</p>
<ul>
  <li>8-bit mode: 64 lanes</li>
  <li>16-bit mode: 32 lanes</li>
  <li>32-bit mode: 16 lanes</li>
  <li>64-bit mode: 8 lanes</li>
</ul>
<p>
The physical implementation uses <strong>64 independent 8-bit slices</strong>. Wider modes “gang” slices:
</p>
<ul>
  <li>16-bit: pairs of slices cooperate (carry flows inside 2-byte groups).</li>
  <li>32-bit: groups of 4 slices.</li>
  <li>64-bit: groups of 8 slices.</li>
</ul>
<p>
A <code>TMODE</code>-driven decoder controls which slice boundaries pass carry and how saturation limits are applied.
This keeps each slice’s critical path short while still supporting all element widths.
</p>

<h3>2.2 Single-cycle TALU</h3>
<p>
For a typical <code>TALU</code> operation (ADD/SUB/AND/OR/XOR/MIN/MAX/ABS):
</p>
<ol>
  <li>Read two source tiles from the tile register file (TRF) into 512-bit buses.</li>
  <li>Select per-lane inputs via small 2:1 or 4:1 muxes (depending on operand selector <code>S</code>).</li>
  <li>Run through the 64×8-bit ALU slices with limited carry propagation inside groups.</li>
  <li>Write the 512-bit result back into a TRF entry.</li>
</ol>
<p>
Logic depth per lane is essentially:
<code>mux → 8-bit ALU (+ local carry) → output driver</code>. With 7 nm libraries and careful floorplanning,
this is compatible with a 1 GHz core clock for these simple ops.
</p>

<h3>2.3 Single-cycle TRED (reductions)</h3>
<p>
For <code>TRED.SUM</code> in 8-bit mode:
</p>
<ul>
  <li>Inputs: 64 values, each in [-128, +127] or [0, 255].</li>
  <li>Max magnitude (unsigned) per tile: 64 × 255 = 16,320 (< 2<sup>15</sup>).</li>
</ul>
<p>
Implementation:
</p>
<ul>
  <li>Each lane sign- or zero-extends its 8-bit value to a modest width (e.g. 16 bits).</li>
  <li>A <strong>carry-save adder (CSA) tree</strong> combines the 64 inputs over ~4 stages.</li>
  <li>A final carry-propagate adder produces a 256-bit result, mapped into <code>ACC0–ACC3</code>.</li>
</ul>
<p>
For <code>TRED.MIN/MAX</code>, a tournament network compares and passes winners up a tree, with depth log₂(64) = 6:
each stage is just a compare + mux, which fits comfortably inside a tile-ALU cycle.
</p>

<h3>2.4 TRANS in one cycle via layout metadata</h3>
<p>
Doing a physical 8×8 transpose with a 64×64 byte crossbar would be expensive. A lighter solution:
</p>
<ul>
  <li>Each tile register has a 1-bit <strong>layout flag</strong>:
    <ul>
      <li><code>layout = 0</code> — normal row-major view.</li>
      <li><code>layout = 1</code> — logical transpose view.</li>
    </ul>
  </li>
  <li>Tile indexing becomes:
    <pre><code>idx(row,col) = (layout == 0)
              ? row*8 + col
              : col*8 + row</code></pre>
  </li>
  <li><code>TSYS TRANS</code> simply toggles the layout flag for the destination tile.</li>
</ul>
<p>
No bytes move; only the index mapping changes. Data stays in place, so <code>TRANS</code> becomes a trivial 1-cycle metadata flip.
</p>

<h3>2.5 CONV3 as a pipelined multi-cycle op</h3>
<p>
<code>TSYS CONV3</code> applies a 3-tap 1-D convolution along each row:
</p>
<pre><code>y[r,c] = k0 * x[r,c-1] + k1 * x[r,c] + k2 * x[r,c+1]</code></pre>
<p>
A realistic 4-stage MEX pipeline:
</p>
<ol>
  <li><strong>Stage 0:</strong> Read source tile from TRF.</li>
  <li><strong>Stage 1:</strong> Generate neighbor views (shift left/right to form x<sub>c-1</sub>, x<sub>c</sub>, x<sub>c+1</sub>).</li>
  <li><strong>Stage 2:</strong> Per-lane multiply–accumulate using signed 8-bit multipliers and adders.</li>
  <li><strong>Stage 3:</strong> Saturate results and write to destination tile register.</li>
</ol>
<p>
Latency is 4 cycles from issue to completed tile, but the pipeline can accept a new tile every cycle,
maintaining the advertised 1 tile cycle⁻¹ throughput for <code>CONV3</code>.
</p>

<h3>2.6 Integration with the scalar core pipeline</h3>
<p>
The scalar core keeps its 4-stage in-order pipeline (IF, ID, EX, WB). MEX instructions behave as follows:
</p>
<ul>
  <li><strong>ID:</strong> Decode as MEX, read relevant CSRs (<code>TSRC0/1, TDST, TMODE, TCTRL</code>).</li>
  <li><strong>EX:</strong> Launch into the MEX pipeline; simple ops occupy 1 stage, complex ops more.</li>
  <li><strong>WB:</strong> Update ACCs/FLAGS/CSRs and retire the instruction.</li>
</ul>
<p>
The core can either:
</p>
<ul>
  <li>Stall the scalar pipeline for the documented MEX latency (simplest), or</li>
  <li>Track hazards on tiles/CSRs and allow unrelated scalar ops to continue while MEX runs (more complex, still in-order).</li>
</ul>

<!-- ——— 3  32 MiB MEGAPAD ORG ——————————————— -->
<h2 id="megapad32">3 32 MiB Megapad Organization (16×2 MiB)</h2>

<h3>3.1 Banking and capacity</h3>
<table>
<thead>
  <tr><th>Parameter</th><th>Value</th><th>Notes</th></tr>
</thead>
<tbody>
  <tr>
    <td>Total Megapad</td>
    <td>32 MiB</td>
    <td>On-die SRAM scratchpad.</td>
  </tr>
  <tr>
    <td>Logical banks</td>
    <td>16</td>
    <td>Bank indices 0–15 via <code>SB</code>.</td>
  </tr>
  <tr>
    <td>Per-bank capacity</td>
    <td>2 MiB</td>
    <td>2,097,152 bytes ≈ 16,777,216 bits.</td>
  </tr>
  <tr>
    <td>Address aperture per bank</td>
    <td>4 MiB</td>
    <td>Upper half unmapped in 32 MiB config; access faults if used.</td>
  </tr>
</tbody>
</table>

<h3>3.2 Bank internals and SRAM macros</h3>
<p>
Each 2 MiB bank can be built from an array of standard SRAM macros. For example:
</p>
<ul>
  <li>Use 256 kB macros (2,097,152 bits) → 8 macros per bank, or</li>
  <li>Use 128 kB macros → 16 macros per bank.</li>
</ul>
<p>
Macros are tiled in a rectangular grid. The key constraint is that a <strong>64-byte tile</strong> should map
cleanly into contiguous addresses inside a small, predictable set of macros, avoiding tile data that straddles
many macros and complicates timing.
</p>

<h3>3.3 Per-bank tile buffers</h3>
<p>
To reconcile wide 512-bit tile operations with narrower SRAM ports, each bank includes a small “front porch”:
</p>
<ul>
  <li><strong>Tile buffer:</strong> e.g. 2–4 entries × 64 bytes, implemented as a tiny SRAM or flops.</li>
  <li><strong>Bank-side ports:</strong> 1–2 read/write ports from the main Megapad SRAM into the tile buffer.</li>
  <li><strong>MEX-side port:</strong> A 512-bit interface from the tile buffer into the MEX tile register file.</li>
</ul>
<p>
Access example:
</p>
<ul>
  <li><code>LD1D/LD2D</code>: bank logic performs several narrow reads from the main SRAM to fill a 64-byte tile buffer,
      then presents it as a single 512-bit word to MEX.</li>
  <li><code>ST1D/ST2D</code>: MEX writes the result tile into the buffer, which then trickles back into SRAM via narrow writes.</li>
</ul>
<p>
This keeps the <strong>main Megapad SRAM</strong> reasonably port-light, while giving the tile engine a clean,
cycle-friendly 512-bit view.
</p>

<h3>3.4 Per-bank accumulators (ACC0–ACC3)</h3>
<p>
The spec defines per-bank accumulators <code>ACC0–ACC3</code>, acting collectively as a 256-bit value and indexed by <code>SB</code>.
Physically, a convenient approach is:
</p>
<ul>
  <li>Implement ACCs as a small 16-row × 256-bit register array inside the MEX region.</li>
  <li>Row index = current <code>SB</code> value.</li>
  <li>Scalar <code>CSRR/CSRW</code> accesses use <code>SB</code> to select the correct row under the hood.</li>
</ul>
<p>
This keeps the ACC wiring short and local to MEX while preserving the architectural “per-bank” semantics.
</p>

<!-- ——— 4  CORE + MEX + MEGAPAD FLOORPLAN ——————————————— -->
<h2 id="floorplan">4 Core + MEX + Megapad Floorplan (32 MiB)</h2>

<h3>4.1 High-level floorplan</h3>
<p>
A plausible die-level layout:
</p>
<pre><code>+------------------------------------------------------+
|                 IO / DDR / GPIO ring                 |
|                                                      |
|  [Bank0] [Bank1] [Bank2] [Bank3]                     |
|                                                      |
|  [Bank4]   [  CORE + CSR + MEX TILE ENGINE ] [Bank5] |
|                                                      |
|  [Bank6] [Bank7] [Bank8] [Bank9]                     |
|                                                      |
|           QSPI / SWD / timers / misc                 |
+------------------------------------------------------+</code></pre>
<p>
Banks 10–15 can continue around the ring or be clustered symmetrically; the critical point is:
</p>
<ul>
  <li>MEX sits centrally, with short 512-bit routes to a subset of nearby banks.</li>
  <li>Banks are grouped into <strong>pods</strong>, each with tile buffers and local routing to MEX.</li>
</ul>

<h3>4.2 MEX / tile engine placement and structure</h3>
<p>
The MEX block sits between the scalar core and the Megapad bank pods. Internally:
</p>
<ul>
  <li><strong>Tile Register File (TRF):</strong> 4–8 tile registers (512-bit each), with:
    <ul>
      <li>2–3 read ports (for A/B operands).</li>
      <li>1 write port (for result tiles).</li>
    </ul>
  </li>
  <li><strong>512-bit ALU:</strong> 64×8-bit slices grouped by <code>TMODE</code>.</li>
  <li><strong>Reduction and compare trees:</strong> For <code>TRED</code> and flag updates.</li>
  <li><strong>ACC array:</strong> 16 rows × 256 bits, indexed by <code>SB</code>.</li>
  <li><strong>Control:</strong> MEX decode, scheduling, and CSR interface (<code>TMODE/TCTRL/TSRC*/TDST</code>).</li>
</ul>

<h3>4.3 Scalar core and CSR block</h3>
<p>
The scalar core is relatively compact:
</p>
<ul>
  <li>16×64-bit GPR file.</li>
  <li>4-stage in-order pipeline (IF, ID, EX, WB).</li>
  <li>64-bit ALU and load/store unit.</li>
</ul>
<p>
Place the core directly adjacent to the MEX block. A shared <strong>CSR block</strong> sits between them,
hosting:
</p>
<ul>
  <li>MEX CSRs: <code>TSRC0, TSRC1, TDST, TMODE, TCTRL, ACCx</code>.</li>
  <li>Megapad cursor: <code>SB, SR, SC, SW</code>.</li>
  <li>System CSRs: <code>FLAGS, IVT_BASE, IVEC_ID, SPSEL</code>, etc.</li>
</ul>
<p>
This keeps scalar–MEX control wiring short and local.
</p>

<!-- ——— 5  ON-CHIP INTERCONNECTS ——————————————— -->
<h2 id="interconnects">5 On-Chip Interconnects</h2>

<h3>5.1 Scalar load/store path</h3>
<p>
A 64-bit scalar load/store bus connects:
</p>
<ul>
  <li>Scalar core ↔ load/store unit.</li>
  <li>Load/store unit ↔ Megapad banks / external DDR / MMIO.</li>
</ul>
<p>
The load/store unit decodes the 64-bit address (Megapad vs external vs CSRs) and forwards
requests to the appropriate target. These are relatively sparse and latency tolerant.
</p>

<h3>5.2 Tile datapath</h3>
<p>
The tile datapath is the performance-critical path:
</p>
<ul>
  <li>512-bit links between bank tile buffers and the MEX TRF.</li>
  <li>Local crossbars inside each bank pod (e.g. 4 banks share a pod-level mux into MEX).</li>
  <li>Short, straight, high-metal routing to reduce RC delay and buffering.</li>
</ul>
<p>
The layout keeps these buses physically short by clustering Megapad bank pods around MEX.
</p>

<h3>5.3 CSR and control wiring</h3>
<p>
CSRs and control signals are narrow but widely used:
</p>
<ul>
  <li>Scalar core writes <code>SB/SR/SC/SW</code> to define tile coordinates.</li>
  <li>MEX reads those CSRs to resolve <code>TileAddr</code> or select ACC rows.</li>
  <li>Banks receive small control fields (bank ID, tile index, load/store commands) from MEX and/or load/store unit.</li>
</ul>
<p>
These run in mid-level routing layers and avoid the wide 512-bit paths used by tile data.
</p>

<!-- ——— 6  EXAMPLE TSYS_CONV3 PATH ——————————————— -->
<h2 id="example">6 Example: TSYS_CONV3 on a Tile in 32 MiB Megapad</h2>

<p>
This example traces a <code>TSYS CONV3</code> instruction acting on a tile in bank 3
(<code>SB=3</code>) with the 32 MiB layout (2 MiB per bank).
</p>

<ol>
  <li><strong>Setup (scalar side):</strong>
    <ul>
      <li>Firmware sets <code>SB=3</code>, <code>SR/SC/SW</code> to pick a tile.</li>
      <li>Firmware writes <code>TSRC0</code> and <code>TDST</code> with base addresses of source and destination tiles.</li>
      <li>Convolution taps <code>K0..K2</code> are written via <code>CSRW</code>.</li>
    </ul>
  </li>

  <li><strong>Tile load from bank 3:</strong>
    <ul>
      <li><code>LD1D</code> is issued for the source tile.</li>
      <li>Bank 3’s decode logic maps the tile index
        (<code>TileIdx = SR*SW + SC</code>) into macro row/column addresses.</li>
      <li>Several narrow SRAM reads fill a 64-byte tile buffer local to bank 3.</li>
      <li>The 512-bit tile buffer content is pushed into a TRF slot (e.g. <code>T0</code>).</li>
    </ul>
  </li>

  <li><strong>TSYS CONV3 execution (4-cycle MEX pipeline):</strong>
    <ul>
      <li><strong>Cycle 0:</strong> <code>TSYS CONV3</code> is decoded, <code>T0</code> is read from TRF.</li>
      <li><strong>Cycle 1:</strong> MEX forms neighbor views for each row (shifted left/right). </li>
      <li><strong>Cycle 2:</strong> Per-lane MACs apply <code>K0..K2</code> with saturation.</li>
      <li><strong>Cycle 3:</strong> Result lanes are packed into a destination tile register (e.g. <code>TD</code>).</li>
    </ul>
  </li>

  <li><strong>Tile store back to bank 3:</strong>
    <ul>
      <li>A <code>ST1D</code> instruction writes <code>TD</code> into bank 3’s tile buffer.</li>
      <li>The bank logic streams the 64 bytes into the main 2 MiB SRAM via narrow writes.</li>
    </ul>
  </li>
</ol>

<p>
Once the MEX pipeline is full, new tiles can be issued to <code>TSYS CONV3</code> each cycle,
achieving sustained 1 tile cycle⁻¹ throughput while the tile buffers and Megapad banks run asynchronously over multiple cycles beneath that abstraction.
</p>

<p class="legal">
This document is an internal architectural note intended to sanity-check timing and floorplanning assumptions for a 32 MiB Megapad-64 configuration. It does not represent a final physical implementation and omits detailed transistor-level analysis, signoff timing, and IR-drop considerations.
</p>

</body>
</html>
